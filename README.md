# Multi-Agent reinforcement learning for multi-drone transport system

[![IsaacSim](https://img.shields.io/badge/IsaacSim-4.2.0-silver.svg)](https://docs.omniverse.nvidia.com/isaacsim/latest/overview.html)
[![Isaac Lab](https://img.shields.io/badge/IsaacLab-1.0.0-silver)](https://isaac-sim.github.io/IsaacLab)
[![Python](https://img.shields.io/badge/python-3.10-blue.svg)](https://docs.python.org/3/whatsnew/3.10.html)
[![Linux platform](https://img.shields.io/badge/platform-linux--64-orange.svg)](https://releases.ubuntu.com/20.04/)
[![Windows platform](https://img.shields.io/badge/platform-windows--64-orange.svg)](https://www.microsoft.com/en-us/)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://pre-commit.com/)
[![License](https://img.shields.io/badge/license-MIT-yellow.svg)](https://opensource.org/license/mit)

Author - [Jack Zeng](https://github.com/Jackkert)

https://github.com/user-attachments/assets/60a7d0ad-d294-4039-a6ad-df66795bbd31

https://github.com/user-attachments/assets/3d6d4698-90ec-46e8-8f35-58d5e9d59d81

## Overview
This repository is an NVIDIA Isaac Lab extension that contains the environment and algorithm to control a multi-drone transport system (flycrane). As of now, these task have been implemented:

- `hover_llc`/`hover` The flycrane gets a reference pose for the payload where it should hover. This is done with a differential based flatness controller (DFBC), or end-to-end, which, in this project, is from the simulation states to the forces and torques on the quadrotors. All tasks hereafter include the DFBC to minimize the sim2real gap.

- `track` Track a reference trajectory which is generated by an [external script](https://github.com/Jackkert/trajectory_generator). Different trajectories is figure-8 or ellipsoid shapes are generated for the payload.

- `FlyThrough` Make the payload fly through a gap/between 2 walls, avoiding collisions.

### Installation

- Install Isaac Lab, see the [installation guide](https://isaac-sim.github.io/IsaacLab/source/setup/installation/index.html).

- Using a python interpreter that has Isaac Lab installed, install the library

```
cd exts/MARL_mav_carry_ext
python -m pip install -e .
```

## Assets

The assets used for the tasks are under [exts/MARL_mav_carry_ext/MARL_mav_carry_ext/assets](https://github.com/Jackkert/MARL_mav_ext/tree/main/exts/MARL_mav_carry_ext/MARL_mav_carry_ext/assets).
The assets folder contains `$(ROBOT).py` files which have the configuration the respective robot in the form an `ArticulationCfg` (Isaac Lab actuated robot config class).

Then, in the [exts/MARL_mav_carry_ext/MARL_mav_carry_ext/assets/data/AMR](https://github.com/Jackkert/MARL_mav_ext/tree/main/exts/MARL_mav_carry_ext/MARL_mav_carry_ext/assets/data/AMR) folder, the robot URDF and corresponding meshes are located in the `$(ROBOT)_data` folder and the USD files can be found in the `$(ROBOT)` folder.

## Controllers
The controllers are implemented in [exts/MARL_mav_carry_ext/MARL_mav_carry_ext/controllers](https://github.com/Jackkert/MARL_mav_ext/tree/feature/track_task/exts/MARL_mav_carry_ext/MARL_mav_carry_ext/controllers). The DFBC can be found in the `geometric.py`.

## Plotting Tools
The plotting tools are implemented in [exts/MARL_mav_carry_ext/MARL_mav_carry_ext/plotting_tools](https://github.com/Jackkert/MARL_mav_ext/tree/feature/track_task/exts/MARL_mav_carry_ext/MARL_mav_carry_ext/plotting_tools). There is currently a plotting class for `ManagerBasedRLEnv`.

## Environments/Tasks

The environments for each specific flycrane task can be found under [exts/MARL_mav_carry_ext/MARL_mav_carry_ext/tasks/MARL_mav_carry](https://github.com/Jackkert/MARL_mav_ext/tree/main/exts/MARL_mav_carry_ext/MARL_mav_carry_ext/tasks/MARL_mav_carry). *In progress:* an environment to enable a generalist policy to complete all tasks.

Moreover, tasks for a single falcon drone are implemented in [exts/MARL_mav_carry_ext/MARL_mav_carry_ext/tasks/single_falcon](https://github.com/Jackkert/MARL_mav_ext/tree/feature/track_task/exts/MARL_mav_carry_ext/MARL_mav_carry_ext/tasks/single_falcon). This environment was only used to test the controller.

### Environment structure

The configuration class for each task can be found in their respective folder. Following the Isaac Lab structure the environment is implemented as a [`ManagerBasedRLEnv`](https://isaac-sim.github.io/IsaacLab/source/api/lab/omni.isaac.lab.envs.html#omni.isaac.lab.envs.ManagerBasedRLEnv). Later, the environment will likely be rewritten to the [`DirectMARLEnv`](https://isaac-sim.github.io/IsaacLab/source/api/lab/omni.isaac.lab.envs.html#omni.isaac.lab.envs.DirectMARLEnv) class.

The manager based environment consists of multiple modules, and their configs can be found in the `$(TASK)_env_cfg.py` file:

- `Scene` The scene that describes the environment. This describes all prims present in the environment such as ground plane, lights, robots and sensors. Also describes obstacles if there are any in the environment.
- `CommandManager` Generate a new pose command as a reference for the payload and resample at certain time intervals. Or in the case of a trajectory, sample a new trajectory and pass pass the 4 (variable) future points as observations.
- `ActionManager` Processes and sends the actions to the simulation. For the end-to-end case, this class only has 1 `ActionTerm` that purely reshapes and clamps the forces and torques applied to the body. The RL policy directly learns the forces and torques on the drone bodies (collective thrust and 3 torques). When using a lower level controller, the policy output is passed to the controller and mapped to rotor forces here.
- `ObservationManager` The observations available to the policy. Currently, the problem is handled as a centralized problem and contains the following observations:
    - Payload state (positions, orientations, linear/angular velocities and accelerations) in environment frame
    - Drone states (positions, orientations, linear/angular velocities and accelerations) in environment frame
    - Payload pose errors to the goal
    - Payload twist errors to the goal
    - Payload acceleration errors to the goal
    - Drone positions relative to eachother
    - Euclideans distance between the drones
    - Angles between cables and payload
- `EventManager` Describes what happens on certain events such as startup, reset or at certain time intervals. Right now, when reset is called, velocities and external forces/torques on all bodies are set to 0. The pose of the flycrane is sampled from a uniform distribution to randomize the initial state.
- `RewardManager` Implements the reward function. Consisting of the following terms:
    - `reward_pose` Reward for tracking the payload pose
    - `reward_twist` Reward for tracking the payload twist
    - `reward_force` Reward to keep the effort small (rotor forces)
    - `reward_force_smoothness` Keep the changes in force over with respect to the previous timestep small
    - `reward_policy_action_smoothness` Keep the changes in policy output over with respect to the previous timestep small
    - `reward_downwash` Reward for keeping the wake of the drones away from the payload

- `TerminationsManager` Terminates the episode corresponding to an environment if a termination condition has been met. The implemented termination terms are"
    - `time_out` Time out after max episode length
    - `falcon_fly_low` Terminate when drones fly too low
    - `payload_fly_low` Terminate when the payload flies too low
    - `illegal_contact` Terminate when forces between bodies get too large
    - `payload_cable_angle` Terminate when the angle between the cables and the payload get too large
    - `drone_cable_angle` Terminate when the angle between the cables and the drones get too large
    - `large_states` Terminate when any states of any body are too large
    - `bounding_box` Terminate when the articulation goes outside of a specified area
    - `cables_collide` Terminate when the cables collide
    - `drones_collide` Terminate when the drones get too close to each other
    - `target_too_far` Terminate when the pose target is too far (used in trajectory case)

- `CurriculumManager` Manager of different learning tasks in order to increase difficulty of the task over time (curriculum learning). This is not implemented.

## Training and playing
Isaac Lab offers different wrappers for different RL libraries to make it easy to switch between libraries. The scripts for the corresponding libraries are implemented in [scripts](https://github.com/Jackkert/MARL_mav_ext/tree/main/scripts). The useable libraries are rl_games, rsl_rl and skrl.

### Agents
The agent configurations for the flycrane are in the respective task's `config/flycrane/agents`. folder The environments are registered as a gym environment and the parameters of the agents can be changed here.

### Training
To train the agent, for example using skrl. You can run the following command from the command line:

`python3 scripts/skrl/train.py --task=Isaac-flycrane-payload-track-v0 --headless --num_envs=4096 --seed=-1`

This will start the training for the hover task with the configured settings in the agent configuration file. For more command line interface arguments, check the respective `train.py` file under `scripts/`.


### Playing
To play with the learned agent, you can run the `play.py` script. This will load the latest checkpoint from the `logs` that have been accumulated during training. For this, execute (for example):

`python3 scripts/skrl/play.py --task=Isaac-flycrane-payload-track-v0 --headless --video --video_length=800 --save_plots --checkpoint=$(PATH_TO_PT_FILE)`

To gather data and plot results of the played episode, add the `--save_plots` flag, this will plot several statistics against time such as metrics (for each task), payload and drone states over time.

## Code formatting

We have a pre-commit template to automatically format your code.
To install pre-commit:

```bash
pip install pre-commit
```

Then you can run pre-commit with:

```bash
pre-commit run --all-files
```
